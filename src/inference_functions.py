"""
Inference functions
===================

.. module:: inference_fuctions
   :platform: Linux
   :synopsis: Functions that performs the inference process.

This script contains the functions that perform inference and manage the loading of the models, the datasets and the
computation of the correctness.

.. autosummary::
   :toctree: generated/

   inference
   text_generation
   image_classification
   code_generation
   infer_text_generation
   infer_image_classification
"""

# Frameworks
import torch
import tensorflow as tf

# Emissions and energy consumption
from codecarbon import track_emissions

# Inference pipeline
from transformers import pipeline

# Data transformation
import ast

# Time calculation
import time

# File modification
import os

# Image loading
from PIL import Image

# Auxiliary functions
from inference_utils import load_model, language_model_score, pep8_score


@track_emissions
def inference(model_name: str, model_short_name: str) -> float:
    """
    Divides the models by domains, calls the respective function and returns the average correctness.

    :param model_name: Model name.
    :param model_short_name: Short model name (eg. 'gpt2').
    :return: Average correctness.
    """
    print("#############################################################################################")
    print(f'Inference for {model_name}')
    print("#############################################################################################")

    # Model loading
    model, tokenizer, framework = load_model(model_short_name, 'models/saved/' + model_name)

    # Call inference functions depending on domain
    if model_short_name in ['gpt2', 'opt', 'gptj']:
        correctness = text_generation(model_name=model_name, model=model, tokenizer=tokenizer, framework=framework)
    elif model_short_name in ['resnet', 'vit', 'regnet']:
        # Read the labels file
        with open('../data/imagenet1000_idx_to_labels.txt', 'r') as f:
            labels = f.read()

        imagenet_labels = ast.literal_eval(labels)
        correctness = image_classification(model_name=model_name, model=model, processor=tokenizer, framework=framework,
                                           labels=imagenet_labels)
    else:
        correctness = code_generation(model_name=model_name, model=model, tokenizer=tokenizer, framework=framework)
    print(f'Correctness {model_name}: {correctness}')
    # Return the average correctness
    return sum(correctness) / len(correctness)


def text_generation(model_name: str, model, tokenizer, framework: str) -> list[float]:
    """
    Infers the text generation models and returns the list of correctness.

    :param model_name: Model name.
    :param model: Model to infer.
    :param tokenizer: Model tokenizer.
    :param framework: ML framework.
    :return: List of correctness for each data point.
    """
    i = 0
    score = []
    with open('../data/text_dataset.txt', 'r') as dataset:
        # Infer each data point
        for data in dataset:
            # Safely evaluate each line as a dictionary object
            data = ast.literal_eval(data)
            start_time = time.time()
            # Call inference function
            output = infer_text_generation(text=data['text'], model=model, tokenizer=tokenizer, framework=framework,
                                           quantized='quantized' in model_name)
            end_time = time.time()
            elapsed_time = end_time - start_time
            print("#############################################################################################")
            print(f'Iteration: {i + 1} Output: {output} Time taken: {elapsed_time} s')
            print("#############################################################################################")
            # Compute score
            lms = language_model_score(output)
            score.append(lms)
            i = i + 1
    return score


def image_classification(model_name: str, model, processor, framework: str, labels: dict) -> list[float]:
    """
    Infers the image classification models and returns the correctness.

    :param model_name: Model name.
    :param model: Model to infer.
    :param processor: Model processor.
    :param framework: ML framework.
    :param labels: ImageNet labels.
    :return: List of correctness for the data points.
    """
    i = 0
    accuracy = []
    directory_path = '../data/image_dataset/'
    # Read the mapping file to get the image file names and their labels
    mapping_file_path = os.path.join(directory_path, 'mapping.txt')
    with open(mapping_file_path, 'r') as mapping_file:
        # Infer each data point
        for line in mapping_file:
            image_file_name, label = line.strip().split('\t')
            image_path = os.path.join(directory_path, image_file_name)
            # Read the image file
            image = Image.open(image_path)
            start_time = time.time()
            # Call inference function
            output = infer_image_classification(image=image, model=model, processor=processor, framework=framework,
                                                quantized='quantized' in model_name)
            end_time = time.time()
            elapsed_time = end_time - start_time
            print("#############################################################################################")
            print(f'Iteration: {i+1} Output: {output} Target: {labels[int(label)]} Time taken: {elapsed_time} s')
            print("#############################################################################################")
            # Compute accuracy
            accuracy.append(output == labels[int(label)])
            i = i + 1
    return accuracy


def code_generation(model_name: str, model, tokenizer, framework: str) -> list[float]:
    """
    Infers the code generation models and returns the correctness.

    :param model_name: Model name.
    :param model: Model to infer.
    :param tokenizer: Model tokenizer.
    :param framework: ML framework.
    :return: List of correctness for the data points.
    """
    i = 0
    code_quality = []
    with open('../data/code_dataset.txt', 'r') as dataset:
        # Infer each data point
        for data in dataset:
            start_time = time.time()
            # Call inference function
            output = infer_text_generation(text=data, model=model, tokenizer=tokenizer, framework=framework,
                                           quantized='quantized' in model_name)
            end_time = time.time()
            elapsed_time = end_time - start_time
            print("#############################################################################################")
            print(f'Iteration: {i + 1} Output: {output} Time taken: {elapsed_time} s')
            print("#############################################################################################")
            # Compute score
            pep_score = pep8_score(output)
            lms = language_model_score(output)
            score = 0.5*pep_score + 0.5*lms
            print(pep_score, lms, score)
            code_quality.append(score)
            i = i + 1
    return code_quality


def infer_text_generation(text: str, model, tokenizer, framework: str, quantized: bool, length: int = 50) -> str:
    """
    Infers a string of text for a given model and returns the generated text.

    :param text: Text to infer.
    :param model: Model to infer.
    :param tokenizer: Model tokenizer.
    :param framework: ML framework.
    :param quantized: Whether it is the quantized version of the model.
    :param length: Length of the generated text.
    :return: Generated text.
    """
    if framework == 'pt':
        # PyTorch inference
        generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

        output = generator(text, max_length=length)

        return output[0]["generated_text"]
    else:
        # TensorFlow inference
        if quantized:
            generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
            return generator(text)[0]['generated_text']
        else:
            inputs = tokenizer(text, return_tensors="tf")
            outputs = model(inputs)
            logits = outputs.logits

            # Get the index of the highest probability token
            predicted_token_indexes = tf.argmax(logits, axis=-1)[0]

            # Decode the generated text
            output_text = tokenizer.decode(predicted_token_indexes)

            return text + output_text


def infer_image_classification(image: Image.Image, model, processor, framework: str, quantized: bool) -> str:
    """
    Infers an image for a given model and returns the output label.

    :param image: Image to infer.
    :param model: Model to infer.
    :param processor: Model processor.
    :param framework: ML framework.
    :param quantized: Whether it is the quantized version of the model.
    :return: Image label.
    """
    inputs = processor(image, return_tensors=framework)

    if framework == 'pt':
        # PyTorch inference
        with torch.no_grad():
            logits = model(**inputs).logits

        # model predicts one of the 1000 ImageNet classes
        predicted_label = logits.argmax(-1).item()
        return model.config.id2label[predicted_label]
    else:
        # TensorFlow inference
        if quantized:
            classifier = pipeline("image-classification", model=model, image_processor=processor)
            return classifier(image)[0]['label']
        else:
            # Run inference on the input image
            logits = model(inputs)[0]

            # Model predicts one of the 1000 ImageNet classes
            predicted_label = tf.argmax(logits, axis=-1).numpy()[0]
            return model.config.id2label[predicted_label]
